{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75eec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import cv2\n",
    "from datasets import load_dataset\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc6ece",
   "metadata": {},
   "source": [
    "# Functions used to retrieve image examples, format into a huggingface dataset, and push to a huggingface repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45a69c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_class_images(class_prompt, class_data_dir, num_class_images):\n",
    "    cmd = f\"python /Users/megan.bultema/Documents/diffusers/examples/custom_diffusion/retrieve.py --class_prompt {class_prompt} --class_data_dir {class_data_dir} --num_class_images {num_class_images}\"\n",
    "    subprocess.run(cmd, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0f86a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_csv_and_push_to_hub(root: str, HF_user = 'megantron'):\n",
    "    \"\"\"\n",
    "    Create a metadata.csv file and push it to the huggingface hub for a given image folder and text file. \n",
    "    This function creates a DataFrame with the image and text data from the text file and writes it to \n",
    "    metadata.csv. It also pushes the dataset to the Hugging Face hub.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    root : str\n",
    "        Absolute path to the folder containing download of images using retrieve_class_images()\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    img_folder = root+'/images/'\n",
    "    img_txtfile = root+'/images.txt'\n",
    "    txt_file = root+'/caption.txt'\n",
    "    # create a list of image file names from the folder\n",
    "    img_files = os.listdir(img_folder)\n",
    "\n",
    "    # create an empty list to store the image data\n",
    "    img_data = [x for x in img_files]\n",
    "\n",
    "    # read the text file and split the contents into a list\n",
    "    with open(img_txtfile, 'r') as f:\n",
    "        img_data = f.read().splitlines()\n",
    "\n",
    "    #remove abs path to comply with hugging face upload\n",
    "    img_file = [x.replace(f'{img_folder}','') for x in img_data]\n",
    "\n",
    "    # read the text file and split the contents into a list\n",
    "    with open(txt_file, 'r') as f:\n",
    "        text_data = f.read().splitlines()\n",
    "\n",
    "    \n",
    "    # create a dictionary with the image and text data\n",
    "    data_dict = {'file_name': img_file, 'text': text_data}\n",
    "\n",
    "    # create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    df.to_csv(f'{img_folder}/metadata.csv', index = False)\n",
    "    dataset = load_dataset(\"imagefolder\", data_dir=img_folder)\n",
    "    notebook_login()\n",
    "    dataset.push_to_hub(f\"{HF_user}/{root.split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37a131",
   "metadata": {},
   "source": [
    "# Call above functions to prepare images for LoRA fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8d35159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading real regularization images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieve_class_images(\"simpsons\", \"/Users/megan.bultema/Documents/image_diffusion/hackingtogether-megan/real_reg/simpsons2\", 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6ac1b86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70245a2cbf244f8ba67a0b5b38361a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/megan.bultema/.cache/huggingface/datasets/imagefolder/default-708ea032dc8d4150/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e798a95bdeb54e928fdebc103844530e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ea2010b7494a379e36571f8a4852ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90af172731f345ffa76f154fac61f282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/megan.bultema/.cache/huggingface/datasets/imagefolder/default-708ea032dc8d4150/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a542285b4ebc4856855536c42d2b94fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3e07e7914a45458a9b4ff7a64d97e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358c8a94d22845df97d97d1f49fb07a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544797fc29bc4301acf8364113ea6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79882cb324b14cedac45bb4f219c05f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e493bc71a7441179d142a23ad59e15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_metadata_csv_and_push_to_hub('/Users/megan.bultema/Documents/image_diffusion/hackingtogether-megan/real_reg/AnselAdams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bdbbe",
   "metadata": {},
   "source": [
    "# Function to fine-tune model with LoRa using sripts available in the diffusers repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaec6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_text_to_image_lora(MODEL_NAME, DATASET_NAME, OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Trains a text-to-image generation model using the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        MODEL_NAME (str): The name or path of the pre-trained model to use.\n",
    "        DATASET_NAME (str): The name or path of the dataset to use.\n",
    "        OUTPUT_DIR (str): The output directory to save the trained model to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Set environment variables\n",
    "    os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "    os.environ[\"DATASET_NAME\"] = DATASET_NAME\n",
    "    os.environ[\"OUTPUT_DIR\"] = OUTPUT_DIR\n",
    "\n",
    "    # Launch training script\n",
    "    !accelerate launch /Users/megan.bultema/Documents/diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n",
    "        --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "        --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n",
    "        --resolution=512 --random_flip \\\n",
    "        --train_batch_size=1 \\\n",
    "        --num_train_epochs=10 --checkpointing_steps=500 \\\n",
    "        --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
    "        --seed=42 \\\n",
    "        --output_dir=$OUTPUT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6742e",
   "metadata": {},
   "source": [
    "# Call above function for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63e0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "05/05/2023 13:44:07 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'dynamic_thresholding_ratio', 'thresholding', 'variance_type', 'sample_max_value', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'cross_attention_norm', 'resnet_out_scale_factor', 'addition_embed_type', 'time_embedding_dim', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'encoder_hid_dim', 'class_embeddings_concat', 'mid_block_only_cross_attention', 'time_embedding_act_fn'} was not found in config. Values will be initialized to default values.\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 398/398 [00:00<00:00, 139kB/s]\n",
      "Downloading and preparing dataset None/None to /Users/megan.bultema/.cache/huggingface/datasets/megantron___parquet/megantron--simpsons_captions-21db4a31475a0f49/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/13.6M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 126k/13.6M [00:00<00:12, 1.09MB/s]\u001b[A\n",
      "Downloading data:   4%|â–‰                    | 583k/13.6M [00:00<00:04, 3.01MB/s]\u001b[A\n",
      "Downloading data:  10%|â–ˆâ–ˆ                  | 1.42M/13.6M [00:00<00:02, 5.37MB/s]\u001b[A\n",
      "Downloading data:  17%|â–ˆâ–ˆâ–ˆâ–                | 2.37M/13.6M [00:00<00:01, 6.99MB/s]\u001b[A\n",
      "Downloading data:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 4.04M/13.6M [00:00<00:00, 10.4MB/s]\u001b[A\n",
      "Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 5.68M/13.6M [00:00<00:00, 12.4MB/s]\u001b[A\n",
      "Downloading data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 7.13M/13.6M [00:00<00:00, 13.1MB/s]\u001b[A\n",
      "Downloading data:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 9.17M/13.6M [00:00<00:00, 15.4MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.6M/13.6M [00:01<00:00, 13.1MB/s]\u001b[A\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.35s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 829.24it/s]\n",
      "Dataset parquet downloaded and prepared to /Users/megan.bultema/.cache/huggingface/datasets/megantron___parquet/megantron--simpsons_captions-21db4a31475a0f49/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 214.55it/s]\n",
      "05/05/2023 13:44:15 - INFO - __main__ - ***** Running training *****\n",
      "05/05/2023 13:44:15 - INFO - __main__ -   Num examples = 200\n",
      "05/05/2023 13:44:15 - INFO - __main__ -   Num Epochs = 10\n",
      "05/05/2023 13:44:15 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "05/05/2023 13:44:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "05/05/2023 13:44:15 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/05/2023 13:44:15 - INFO - __main__ -   Total optimization steps = 2000\n",
      "Steps:   0%|                                           | 0/2000 [00:00<?, ?it/s][W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Steps:   0%|   | 6/2000 [07:58<44:06:33, 79.64s/it, lr=0.0001, step_loss=0.0057]/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Steps:  25%|â–Ž| 500/2000 [11:06:10<33:04:14, 79.37s/it, lr=0.0001, step_loss=0.1305/06/2023 00:50:26 - INFO - accelerate.accelerator - Saving current state to deliberate_simpsons200/checkpoint-500\n",
      "05/06/2023 00:50:26 - INFO - accelerate.checkpointing - Model weights saved in deliberate_simpsons200/checkpoint-500/pytorch_model.bin\n",
      "05/06/2023 00:50:26 - INFO - accelerate.checkpointing - Optimizer state saved in deliberate_simpsons200/checkpoint-500/optimizer.bin\n",
      "05/06/2023 00:50:26 - INFO - accelerate.checkpointing - Scheduler state saved in deliberate_simpsons200/checkpoint-500/scheduler.bin\n",
      "05/06/2023 00:50:26 - INFO - accelerate.checkpointing - Random states saved in deliberate_simpsons200/checkpoint-500/random_states_0.pkl\n",
      "05/06/2023 00:50:26 - INFO - __main__ - Saved state to deliberate_simpsons200/checkpoint-500\n",
      "Steps:  50%|â–Œ| 1000/2000 [22:07:35<22:00:43, 79.24s/it, lr=0.0001, step_loss=0.005/06/2023 11:51:50 - INFO - accelerate.accelerator - Saving current state to deliberate_simpsons200/checkpoint-1000\n",
      "05/06/2023 11:51:50 - INFO - accelerate.checkpointing - Model weights saved in deliberate_simpsons200/checkpoint-1000/pytorch_model.bin\n",
      "05/06/2023 11:51:50 - INFO - accelerate.checkpointing - Optimizer state saved in deliberate_simpsons200/checkpoint-1000/optimizer.bin\n",
      "05/06/2023 11:51:50 - INFO - accelerate.checkpointing - Scheduler state saved in deliberate_simpsons200/checkpoint-1000/scheduler.bin\n",
      "05/06/2023 11:51:50 - INFO - accelerate.checkpointing - Random states saved in deliberate_simpsons200/checkpoint-1000/random_states_0.pkl\n",
      "05/06/2023 11:51:50 - INFO - __main__ - Saved state to deliberate_simpsons200/checkpoint-1000\n",
      "Steps:  75%|â–Š| 1500/2000 [33:11:02<11:00:54, 79.31s/it, lr=0.0001, step_loss=0.005/06/2023 22:55:17 - INFO - accelerate.accelerator - Saving current state to deliberate_simpsons200/checkpoint-1500\n",
      "05/06/2023 22:55:17 - INFO - accelerate.checkpointing - Model weights saved in deliberate_simpsons200/checkpoint-1500/pytorch_model.bin\n",
      "05/06/2023 22:55:17 - INFO - accelerate.checkpointing - Optimizer state saved in deliberate_simpsons200/checkpoint-1500/optimizer.bin\n",
      "05/06/2023 22:55:17 - INFO - accelerate.checkpointing - Scheduler state saved in deliberate_simpsons200/checkpoint-1500/scheduler.bin\n",
      "05/06/2023 22:55:17 - INFO - accelerate.checkpointing - Random states saved in deliberate_simpsons200/checkpoint-1500/random_states_0.pkl\n",
      "05/06/2023 22:55:17 - INFO - __main__ - Saved state to deliberate_simpsons200/checkpoint-1500\n",
      "Steps: 100%|â–ˆ| 2000/2000 [44:12:29<00:00, 79.31s/it, lr=0.0001, step_loss=0.004005/07/2023 09:56:44 - INFO - accelerate.accelerator - Saving current state to deliberate_simpsons200/checkpoint-2000\n",
      "05/07/2023 09:56:44 - INFO - accelerate.checkpointing - Model weights saved in deliberate_simpsons200/checkpoint-2000/pytorch_model.bin\n",
      "05/07/2023 09:56:44 - INFO - accelerate.checkpointing - Optimizer state saved in deliberate_simpsons200/checkpoint-2000/optimizer.bin\n",
      "05/07/2023 09:56:44 - INFO - accelerate.checkpointing - Scheduler state saved in deliberate_simpsons200/checkpoint-2000/scheduler.bin\n",
      "05/07/2023 09:56:44 - INFO - accelerate.checkpointing - Random states saved in deliberate_simpsons200/checkpoint-2000/random_states_0.pkl\n",
      "05/07/2023 09:56:44 - INFO - __main__ - Saved state to deliberate_simpsons200/checkpoint-2000\n",
      "Steps: 100%|â–ˆ| 2000/2000 [44:12:29<00:00, 79.31s/it, lr=0.0001, step_loss=0.0729Model weights saved in deliberate_simpsons200/pytorch_lora_weights.bin\n",
      "/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "{'cross_attention_norm', 'resnet_out_scale_factor', 'addition_embed_type', 'time_embedding_dim', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'encoder_hid_dim', 'class_embeddings_concat', 'mid_block_only_cross_attention', 'time_embedding_act_fn'} was not found in config. Values will be initialized to default values.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/megan.bultema/Documents/diffusers/examples/text_to_image/train_text_to_image_lora.py\", line 861, in <module>\n",
      "    main()\n",
      "  File \"/Users/megan.bultema/Documents/diffusers/examples/text_to_image/train_text_to_image_lora.py\", line 840, in main\n",
      "    images.append(pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0])\n",
      "  File \"/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/megan.bultema/Documents/diffusers/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\", line 626, in __call__\n",
      "    self.check_inputs(\n",
      "  File \"/Users/megan.bultema/Documents/diffusers/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\", line 493, in check_inputs\n",
      "    raise ValueError(\n",
      "ValueError: Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 100%|â–ˆ| 2000/2000 [44:12:32<00:00, 79.58s/it, lr=0.0001, step_loss=0.0729\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/megan.bultema/opt/anaconda3/envs/diffpy2/bin/accelerate\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 923, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 579, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/Users/megan.bultema/opt/anaconda3/envs/diffpy2/bin/python3.10', '/Users/megan.bultema/Documents/diffusers/examples/text_to_image/train_text_to_image_lora.py', '--pretrained_model_name_or_path=./converted_model_deliberate', '--dataset_name=megantron/simpsons_captions', '--caption_column=text', '--resolution=512', '--random_flip', '--train_batch_size=1', '--num_train_epochs=10', '--checkpointing_steps=500', '--learning_rate=1e-04', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--seed=42', '--output_dir=deliberate_simpsons200']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "train_text_to_image_lora(\"./converted_model_deliberate\",\"megantron/simpsons_captions\",\"deliberate_simpsons200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b066b2",
   "metadata": {},
   "source": [
    "# Function for generating and saving test images with the new fine-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9676fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_images(model_path: str, num_images, prompt, n_prompt) -> None:\n",
    "    \"\"\"\n",
    "    Generate 5 test images using StableDiffusionPipeline and save them to a local directory named test_images.\n",
    "\n",
    "    Args:\n",
    "    - model_path (str): The path to the pre-trained model directory.\n",
    "    num_images (int): Number of test images to generate\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # check if test_images directory exists, if it does, remove it and create a new one\n",
    "    if os.path.exists(\"test_images\"):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(\"test_images\")\n",
    "\n",
    "    # create StableDiffusionPipeline object and load attention processes\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"./converted_model_deliberate\")\n",
    "    pipe.unet.load_attn_procs(model_path)\n",
    "    pipe.to(\"mps\")\n",
    "\n",
    "    # generate 5 test images and save them to test_images directory\n",
    "    for i in range(num_images):\n",
    "\n",
    "        image = pipe(prompt, negative_prompt=n_prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "        save_header = model_path.split('/')[-1]\n",
    "        image.save(f\"test_images/{save_header}_test{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba93904",
   "metadata": {},
   "source": [
    "# Below are calls to the above function to test various fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44f2ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3964dd60738741e897faa31deff66506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b3bdc8ec964a828a377e50d20e419d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9714e751644538979536933335835d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c5b3e2d6c0484aa364f6bd0d9a19eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f026c2f6006949388d7bb8be1e78ca15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = \"\"\"(Cinematic photo: 1. 3), rat magician, directional look, \n",
    "octane render, ultra detailed, wide angle full body, 8k, ultra-detailed, (backlight:1. 2) intricate, style-empire\"\"\"\n",
    "n = \"\"\"nrealfixer, nfixer, nartfixer, illustration, drawing, 3d, b&w, \n",
    "(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, \n",
    "wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), \n",
    "disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation\"\"\"\n",
    "generate_test_images(\"./deliberate_simpsons\", 5, p, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d3c23b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bca5d6ae22745608f47ab1b77a599a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218cb4687ca84bd8baa44fcf451066c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0da66d67444452a20e9022bf912ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b202116ca3b47de90792becbc841877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02623afc4cc645f98e4584d874d90929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = \"\"\" Mountains style Ansel Adams photography octane render, ultra detailed, wide angle full body, 8k, ultra-detailed, intricate, \"\"\"\n",
    "n = \"\"\"nrealfixer, nfixer, nartfixer, illustration, drawing, 3d, b&w, \n",
    "(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, \n",
    "wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), \n",
    "disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation\"\"\"\n",
    "generate_test_images(\"./deliberate_AnselAdams\", 5, p, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754a8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megan.bultema/opt/anaconda3/envs/diffpy2/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c6c345e1d4457bb4fea7d6c388296a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55d90f1bff44546a9a9b30a766628ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccee95f6862483b8ef8e975266eeb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2691be4adfb443af9d2fddceaae0713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71995a89ad141f889808a69207b8db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = \"\"\" Bart Simpson as a military comander, ultra detailed, wide angle full body, 8k, ultra-detailed, intricate, \"\"\"\n",
    "n = \"\"\"nrealfixer, nfixer, nartfixer, illustration, drawing, 3d, b&w, \n",
    "(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, \n",
    "wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), \n",
    "disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation\"\"\"\n",
    "generate_test_images(\"./deliberate_simpsons200\", 5, p, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b985087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
